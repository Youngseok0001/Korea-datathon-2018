{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korea Datathon Tutorial\n",
    "\n",
    "The aim of this tutorial is to get you familarized with using Colab to run queries with MIMIC database. \n",
    "\n",
    "### environment\n",
    "* python3.6\n",
    "* tensorflow 2.0(beta)\n",
    "* GCP(bigquery)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* SQL(basic)\n",
    "* python3(basic)\n",
    "* statistics(hypothesis testing)\n",
    "* machine-learning(regresison, multi-perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "* Before running any cell in the script, please make sure that there is a green check mark before \"CONNECTED\" on top right corner, if not, please click \"CONNECTED\" button to connect to a random backend.\n",
    "\n",
    "* You can now run the following cell by clicking on the triangle button when you hover over the [ ] space on the top-left corner of the code cell below or Press Shift+Enter.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from itertools import cycle, islice\n",
    "import statsmodels.api as sm\n",
    "from regressors import stats\n",
    "import tensorflow as tf\n",
    "from toolz import *\n",
    "\n",
    "from tableone import TableOne\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a wrapper function that presets some configs\n",
    "def run_query(query):\n",
    "    return pd.io.gbq.read_gbq(query, project_id=\"plasma-block-249708\", verbose=False, configuration={'query':{'useLegacySql': False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample queries\n",
    "\n",
    "1. Select all columns from a **`patients`** table where **subject_id** is \"10006 | 10040 | 10111\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_query(\n",
    "   \"\"\"\n",
    "    select * \n",
    "\n",
    "    from `{db}.patients` \n",
    "    \n",
    "    where \n",
    "        subject_id = 10006 OR\n",
    "        subject_id = 10040 OR\n",
    "        subject_id = 10111\n",
    "    \"\"\"\\\n",
    "    .format(db = \"physionet-data.mimiciii_demo\"))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Inner join the tree tables \"patients\", \"admissions\", \"icustays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = run_query(\n",
    "    \"\"\"\n",
    "    select p.subject_id, a.hadm_id, icustay_id, dob, admittime, dischtime, intime, outtime, ethnicity, gender\n",
    "\n",
    "    from `{db}.patients` p, `{db}.admissions` a, `{db}.icustays` i\n",
    "\n",
    "    where p.subject_id = a.subject_id AND \n",
    "          a.hadm_id    = i.hadm_id\n",
    "    \"\"\"\\\n",
    "    .format(db = \"physionet-data.mimiciii_demo\"))\n",
    "\n",
    "# or...\n",
    "\n",
    "\n",
    "df_2 = run_query(\n",
    "    \"\"\" \n",
    "    select p.subject_id, a.hadm_id, icustay_id, dob, admittime, dischtime, intime, outtime, ethnicity, gender  \n",
    "    from `{db}.patients` p\n",
    "    inner join `{db}.admissions` a USING (subject_id)\n",
    "    inner join `{db}.icustays` i USING (hadm_id)\n",
    "    \"\"\"\\\n",
    "    .format(db = \"physionet-data.mimiciii_demo\")\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "def is_same(df_1, df_2):\n",
    "    if all(df_1 == df_2):print(\"the two dfs are equal\")\n",
    "    else:print(\"the two dfs are not equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **chartevents** table contains charted data such as vital sign measurements. The itemids 211 and 220045 correspond to heart rate (you can double check this in the d_items table).\n",
    "\n",
    "1. find maximum heart-rate of each icustay_id. \n",
    "2. Modify the query to exclude patients with a maximum heart rate of > 140 bpm(possibly data error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1\n",
    "df = run_query(\"\"\"\n",
    "    select icustay_id, max(valuenum) as HeartRate_Max\n",
    "    from `{db}.chartevents`\n",
    "    where itemid in (\n",
    "    \n",
    "        select itemid\n",
    "        from `{db}.d_items`\n",
    "        where lower(label) = 'heart rate'\n",
    "                    )\n",
    "    group by icustay_id\n",
    "      \"\"\"\\\n",
    "    .format(db = \"physionet-data.mimiciii_demo\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2 \n",
    "df = run_query(\"\"\"\n",
    "    select icustay_id, max(valuenum) as HeartRate_Max\n",
    "    from `{db}.chartevents`\n",
    "    where itemid in (\n",
    "    \n",
    "        select itemid\n",
    "        from `{db}.d_items`\n",
    "        where lower(label) = 'heart rate'\n",
    "                    )\n",
    "    group by icustay_id\n",
    "        having HeartRate_Max <= 140\n",
    "      \"\"\"\\\n",
    "    .format(db = \"physionet-data.mimiciii_demo\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that patients in mimiciii data with their **age > 91.5 are masked by shifting thier DOB**. Therefore, **threshold of 91.5 has to be set or simply ignore them in your study**.\n",
    "\n",
    "1. Get the list of subject_id, patient_id with thier primary diagnoses ICD code and their age in years.\n",
    "2. And only take thier first admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_query(\"\"\"\n",
    "\n",
    "     -- first_hadm_id table will store subject_id, hadm_id of patient's first visit.\n",
    "    with first_hadm_id as (\n",
    "    \n",
    "      select subject_id, hadm_id\n",
    "      from(\n",
    "        select subject_id, hadm_id, row_number() over (partition by subject_id order by admittime asc) row_num\n",
    "        from `{db}.admissions` \n",
    "        ) t  \n",
    "      where row_num = 1 \n",
    "      ),\n",
    "      \n",
    "    -- modified_age if age < 91.5 then age else 19.5\n",
    "    modified_age as \n",
    "    (\n",
    "        select subject_id, hadm_id, \n",
    "            case \n",
    "            when date_diff(cast(admittime as date),cast(dob as date),day)/365 < 91.5\n",
    "                then date_diff(cast(admittime as date),cast(dob as date),day)/365\n",
    "            else 91.5\n",
    "            end as age \n",
    "        from `{db}.admissions` a \n",
    "        left join `{db}.patients` p\n",
    "          using (subject_id)\n",
    "    ),\n",
    "    \n",
    "    -- get primary diagnosis of each patient visit.\n",
    "    primary_diag as\n",
    "    (\n",
    "        select subject_id, hadm_id,\n",
    "          (select d_d.short_title\n",
    "           from `{db}.d_icd_diagnoses` d_d\n",
    "           where d_d.icd9_code = d.icd9_code\n",
    "          ) as diagnosis\n",
    "          \n",
    "        from `{db}.diagnoses_icd` d\n",
    "        where seq_num = 1\n",
    "    )\n",
    "    \n",
    "    -- merge all sub queries with columns (subject_id, hadm_id, modified_age,primary_diag)    \n",
    "    select first_hadm_id.*, age, diagnosis\n",
    "    from primary_diag\n",
    "    left join modified_age using(subject_id, hadm_id)\n",
    "    left join first_hadm_id using(subject_id, hadm_id)\n",
    "    \n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\"))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Workflow \n",
    "\n",
    "In general, data extraction from MIMICiii data can be divived in to several steps. tables created from each step can be merged to generate a bigger table which will be used for our final analysis. \n",
    "\n",
    "1. Cohort selection + demographics \n",
    "2. Outcome of interest\n",
    "2. Diagnosis\n",
    "3. Labtest results\n",
    "4. VitalSign\n",
    "5. Merge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY: Determinants of length of stay \n",
    "Lets make a dummy cohort. We will observe what covaraites are responsible for their lenght of stay.\n",
    "\n",
    "1. Cohort selection + demographics : entire cohort(first admit), age(year), gender(M|F).\n",
    "2. Outcome of interest : length of stay(days), length of stay(long|short).\n",
    "3. Diagnosis : number of diagnosis.\n",
    "4. Labtest results: White_blood_cell_mean(24H), Hemoglobin_mean(24H)\n",
    "5. VitalSign: heart_rate_mean(24H), Temperature_max(24H), Temperature_min(24H)\n",
    "6. Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a dictionary object to store queries of each.\n",
    "queries = \\\n",
    "    {\"cohort_demo\" : None, # entire cohort(first admit), age(year), gender(M|F).\n",
    "     \"outcome\"     : None, # length of stay(days), length of stay(long|short).\n",
    "     \"diagnosis\"   : None, # number of diagnosis.\n",
    "     \"lab\"         : None, #  White_blood_cell_mean(24H), Hemoglobin_mean(24H)\n",
    "     \"vital\"       : None} #  heart_rate_mean(24H), Temperature_max(24H), Temperature_min(24H) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cohort selection + demographics : entire cohort(first admit), age(year), gender(M|F). ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[\"cohort_demo\"] = \\\n",
    "\"\"\"\n",
    "    -- first_hadm_id table will store subject_id, hadm_id of patient's first visit.\n",
    "    with first_hadm_id as (\n",
    "    \n",
    "      select subject_id, hadm_id\n",
    "      from(\n",
    "        select subject_id, hadm_id, row_number() over (partition by subject_id order by admittime asc) row_num\n",
    "        from `{db}.admissions` \n",
    "        ) t  \n",
    "      where row_num = 1 \n",
    "      ),\n",
    "      \n",
    "    -- modified_age if age < 91.5 then age else 19.5\n",
    "    modified_age as \n",
    "    (\n",
    "        select subject_id, hadm_id, \n",
    "            case \n",
    "            when date_diff(cast(admittime as date),cast(dob as date),day)/365 < 91.5\n",
    "                then date_diff(cast(admittime as date),cast(dob as date),day)/365\n",
    "            else 91.5\n",
    "            end as age \n",
    "        from `{db}.admissions` a \n",
    "        left join `{db}.patients` p\n",
    "          using (subject_id)\n",
    "    ),\n",
    "    \n",
    "    -- gender. if M then 1 else 0 \n",
    "    \n",
    "    gender as \n",
    "    (\n",
    "        select subject_id, \n",
    "            case when gender = 'M' then 1 else 0 end as sex\n",
    "        from `{db}.patients`\n",
    "    )\n",
    "    \n",
    "    -- merge all queries\n",
    "    select first_hadm_id.*, age, sex\n",
    "    from first_hadm_id\n",
    "    left join modified_age using(subject_id, hadm_id)\n",
    "    left join gender using(subject_id)\n",
    "    \n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Outcome of interest : length of stay(days), length of stay(long|short). ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[\"outcome\"] = \\\n",
    "\"\"\"\n",
    "with length_of_stay as (\n",
    "    select subject_id, hadm_id, icustay_id, \n",
    "        datetime_diff(icu.outtime, icu.intime, hour) AS icu_length_of_stay,\n",
    "        row_number() over (partition by subject_id order by icu.intime asc) row_num\n",
    "    from `{db}.icustays` as icu)\n",
    "    \n",
    "select subject_id, hadm_id, icu_length_of_stay as stay_con, \n",
    "       case \n",
    "       when icu_length_of_stay > 100  then 1\n",
    "       else 0 end as stay_bi\n",
    "from length_of_stay\n",
    "where row_num = 1\n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Diagnosis : number of diagnosis. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[\"diagnosis\"] = \\\n",
    "\"\"\"\n",
    "select hadm_id, count(icd9_code) as n_diag\n",
    "from `{db}.diagnoses_icd`\n",
    "group by hadm_id\n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Labtest results: White_blood_cell_mean(24H), Hemoglobin_mean(24H). ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[\"lab\"] = \\\n",
    "\"\"\"\n",
    "with lab_tests as (\n",
    "    select hadm_id, charttime, value, valuenum,\n",
    "           case when itemid in (50811,51222) then 'hemoglobin'\n",
    "                when itemid in (50822,50971) then 'potassium'\n",
    "                else null end as label                     \n",
    "    from `{db}.labevents`),\n",
    "    \n",
    "lab_tests_prune as (\n",
    "    select subject_id, hadm_id, icustay_id, label, valuenum, charttime \n",
    "    from `{db}.icustays` \n",
    "    left join lab_tests lab using (hadm_id)\n",
    "    where charttime between intime and datetime_add(intime,interval 1 day)\n",
    "          and label is not null\n",
    "          and valuenum is not null\n",
    "          and valuenum > 0)\n",
    "\n",
    "select hadm_id, icustay_id,\n",
    "   avg(case when label = 'hemoglobin' then  valuenum else null end) as hemoglobin_avg,\n",
    "   avg(case when label = 'potassium'  then  valuenum else null end) as troponin_avg\n",
    "from lab_tests_prune\n",
    "group by hadm_id, icustay_id\n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VitalSign: Heart_Rate_mean(24H), Temperature_max(24H), Temperature_min(24H). ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[\"vital\"] = \\\n",
    "\"\"\"\n",
    "with vital_tests as (\n",
    "    select hadm_id, charttime, value, valuenum,\n",
    "           case when itemid in (223762,676,223761,678) then 'temperature'\n",
    "                when itemid in (211,220045) then 'blood_pressure'\n",
    "                else null end as label                     \n",
    "    from `{db}.chartevents`),\n",
    "    \n",
    "vital_tests_prune as (\n",
    "    select subject_id, hadm_id, icustay_id, label, valuenum, charttime \n",
    "    from `{db}.icustays` \n",
    "    left join vital_tests lab using (hadm_id)\n",
    "    where charttime between intime and datetime_add(intime,interval 1 day)\n",
    "          and label is not null\n",
    "          and valuenum is not null\n",
    "          and valuenum > 0)\n",
    "\n",
    "select hadm_id, icustay_id,\n",
    "   max(case when label = 'temperature' then  valuenum else null end) as temperature_max,\n",
    "   min(case when label = 'temperature' then  valuenum else null end) as temperature_min,\n",
    "   avg(case when label = 'blood_pressure' then   valuenum else null end) as blood_pressure_mean\n",
    "from vital_tests_prune\n",
    "group by hadm_id, icustay_id\n",
    "\"\"\"\\\n",
    ".format(db = \"physionet-data.mimiciii_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merge. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = map(run_query, queries.values())\n",
    "df = reduce(lambda x,y : pd.merge(x,y), tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for correlation analysis\n",
    "\n",
    "We may want to find the correlation between some input variables and outcome varialbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which column contains na?\n",
    "na_columns = \\\n",
    "[(c_name, df[c_name].dtypes)\n",
    "     for c_name \n",
    "     in df.columns\n",
    "     if any(pd.isnull(df[c_name]))]\n",
    "\n",
    "print(na_columns)\n",
    "\n",
    "# maen imputation\n",
    "for na_col in map(lambda x : x[0],na_columns):\n",
    "    df[na_col] = df[na_col].fillna((df[na_col].mean()))\n",
    "    \n",
    "    \n",
    "# select columns of interest\n",
    "X_name = ['hemoglobin_avg',\n",
    "          'n_diag',\n",
    "          'troponin_avg',\n",
    "          'age',\n",
    "          'sex',\n",
    "          'temperature_min',\n",
    "          'blood_pressure_mean',\n",
    "          'temperature_max',]\n",
    "\n",
    "Y_name = \"stay_con\"\n",
    "\n",
    "X  = df[X_name]; y = df[Y_name]\n",
    "# Linear regression\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X,y)\n",
    "\n",
    "# To calculate the p-values of beta coefficients: \n",
    "print(\"coef_pval:\\n\", stats.coef_pval(ols, X, y))\n",
    "\n",
    "# to print summary table:\n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "xlabels = X_name\n",
    "stats.summary(ols, X, y, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for correlation analysis\n",
    "\n",
    "We may want to find the correlation between some input variables and outcome varialbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns of interest\n",
    "X_name = ['hemoglobin_avg',\n",
    "          'n_diag',\n",
    "          'troponin_avg',\n",
    "          'age',\n",
    "          'sex',\n",
    "          'temperature_min',\n",
    "          'blood_pressure_mean',\n",
    "          'temperature_max',]\n",
    "\n",
    "Y_name = \"stay_bi\"\n",
    "\n",
    "X  = df[X_name]; y = df[Y_name]\n",
    "# Linear regression\n",
    "logit = sm.Logit(y,X)\n",
    "# fit the model\n",
    "result = logit.fit()\n",
    "# to print summary table:\n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Net for prediction\n",
    "\n",
    "We may want to come out with a prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    input_size    = 8  \n",
    "    train_len     = 80    \n",
    "    output_size   = 2 \n",
    "    learning_rate = 10**-5\n",
    "    batch_size    = 2\n",
    "    epoch         = 10\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "preprocessing tools\n",
    "\"\"\" \n",
    "set_format       = np.float32\n",
    "normalise        = lambda xs : (xs - np.mean(xs))/np.var(xs)\n",
    "add_random_noise = lambda xs : xs + np.random.normal(0,0.001,len(xs)) \n",
    "\n",
    "preprocess = compose(add_random_noise,normalise,set_format)\n",
    "\n",
    "\"\"\"\n",
    "data control tools\n",
    "\"\"\"\n",
    "data_gen = lambda X : list(map(list,zip(*([X] * config.batch_size))))\n",
    "\n",
    "\"\"\"\n",
    "model\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(12, activation=tf.nn.relu, input_shape=(config.input_size,)),  # input shape required\n",
    "  tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(2, activation = tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.weights)\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X.values\n",
    "y_val = y.values\n",
    "\n",
    "X_train, y_train = X_val[:config.train_len], y_val[:config.train_len]\n",
    "X_test,  y_test  = X_val[config.train_len:], y_val[config.train_len:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "for epoch in range(config.epoch):\n",
    "\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for _x, _y in zip(data_gen(map(preprocess, X_train)),\n",
    "                      data_gen(y_train)):\n",
    "        \n",
    "        x_np = np.array(_x)\n",
    "        y_np = np.array(_y)\n",
    "        \n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x_np, y_np)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        epoch_accuracy(y_np, model(x_np))\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "      print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                  epoch_loss_avg.result(),\n",
    "                                                                  epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model configurations\n",
    "\"\"\"\n",
    "\n",
    "class config():\n",
    "    input_size    = 8  \n",
    "    train_len     = 80    \n",
    "    output_size   = 2 \n",
    "    learning_rate = 10**-5\n",
    "    batch_size    = 10\n",
    "    epoch         = 10\n",
    "    model_spec    = \\\n",
    "        {\n",
    "        \"hidden1\": 12,\n",
    "        \"hidden2\": 32,\n",
    "        \"hidden3\": 12,\n",
    "        \"output\" : output_size        \n",
    "        }\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "preprocessing tools\n",
    "\"\"\" \n",
    "set_format       = np.float32\n",
    "normalise        = lambda xs : (xs - np.mean(xs))/np.var(xs)\n",
    "add_random_noise = lambda xs : xs + np.random.normal(0,0.01,len(xs)) \n",
    "\n",
    "preprocess = compose(add_random_noise,normalise,set_format)\n",
    "\n",
    "\"\"\"\n",
    "data control tools\n",
    "\"\"\"\n",
    "data_gen = lambda X : list(map(list,zip(*([X] * config.batch_size))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# initialise weights\n",
    "def init_weights(config):\n",
    "    \n",
    "    \n",
    "    out_perceptron = list(config.model_spec.values())\n",
    "    in_perceptron  = [config.input_size] + out_perceptron[:-1]\n",
    "    \n",
    "    w_dimensions   = list(zip(in_perceptron, out_perceptron))\n",
    "    b_dimensions   = out_perceptron\n",
    "    \n",
    "    Ws, bs = [], []\n",
    "    \n",
    "    for w_dim, b_dim in zip(w_dimensions, b_dimensions):\n",
    "        Ws.append(tf.Variable(tf.random.normal(mean = 0.,stddev = 0.01, shape = w_dim)))\n",
    "        bs.append(tf.Variable(tf.zeros(shape = b_dim)))\n",
    "        \n",
    "    return list(map(list, zip(Ws, bs)))\n",
    "\n",
    "# layers\n",
    "FC      = lambda X, W ,b: (X @ W) + b\n",
    "RELU    = lambda X: tf.where(X>0, X, 0)\n",
    "SIGMOID = lambda X: 1/(1 + tf.exp(-X))\n",
    "\n",
    "FC_RELU_LAYER  = compose(RELU, FC)\n",
    "SIGMOID_LAYER  = compose(SIGMOID, FC)\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,config):\n",
    "        \n",
    "        self.config  = config \n",
    "        self.weights = init_weights(self.config)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "           \n",
    "        hidden_weights  = self.weights[:-1]\n",
    "        sigmoid_weights = self.weights[-1]\n",
    "        \n",
    "        for w, b in hidden_weights:\n",
    "            X = FC_RELU_LAYER(X,w,b)\n",
    "            \n",
    "        prediction = SIGMOID_LAYER(X,*sigmoid_weights)\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.weights)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
